# -*- coding: utf-8 -*-
"""
Created on Tue Jan 14 13:21:12 2025

@author: ynyang94
"""
from pathlib import Path
import matplotlib.pyplot as plt
from matplotlib import rcParams
import numpy as np
import pandas as pd
from pandas import DataFrame
import csv
from fractions import Fraction
from pandas.plotting import scatter_matrix
import scipy.stats
from scipy.stats.mstats import winsorize
import random
import torch
#import torchvision
import torch.nn as nn
import torch.nn.functional as F
import torchvision
from torchvision import transforms
#import torchvision.transforms as T
# from torchvision.io import read_image
# from torchvision.models import resnet18, ResNet18_Weights
import copy
import os
# import cvxpy as cp
import pdb
import math
import sys
sys.setrecursionlimit(2000)

class LeNetfDRO(nn.Module):
    def __init__(self, input_dim=(1, 28, 28), num_classes=10):
    # Primal variable weight aka x in paper should be in size (1,n)
    # set default variable: learning rate; stopping criteria; maximal iteration.
        super(LeNetfDRO, self).__init__()
        self.device = torch.device("cpu")
        self.input_dim = input_dim
        self.num_classes = num_classes

        # Define layers
        self.conv1 = nn.Conv2d(in_channels=input_dim[0], out_channels=6, kernel_size=5, stride=1, padding=2)
        self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1)
        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)
        # Calculate the size of the input to fc1
        dummy_input = torch.zeros(1, *input_dim)
        out = self.pool(nn.functional.relu(self.conv2(self.pool(nn.functional.relu(self.conv1(dummy_input))))))
        flattened_size = out.numel()
 
        self.fc1 = nn.Linear(flattened_size, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, num_classes)

        # Optionally initialize weights
        self._initialize_weights()
        
        self.lr1 = 1e-3
        # Regularized Primal Problem.
        self.regularization = 1
        self.outer_loop = 10
        # Sinkhorn distance regularization
        self.epsilon = 1
        self.noise = 0.1
        self.num_xi = 1
        self.device = torch.device("cpu")
        # initialize x and \eta
        #self.x = torch.randn(10, requires_grad=True)  # Example 10-dimensional vector
        #self.eta = torch.tensor(1.0, requires_grad=True)  # Scalar eta
    
    
    def _initialize_weights(self):
        """
        Initializes weights for the model.
        """
        for module in self.modules():
            if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):
                nn.init.kaiming_normal_(module.weight, mode='fan_out', nonlinearity='relu')
                if module.bias is not None:
                    nn.init.constant_(module.bias, 0)

    
    # Forward Pass
    def forward(self, xi):
        xi = F.relu(self.conv1(xi))
        xi = self.pool(xi)
        xi = F.relu(self.conv2(xi))
        xi = self.pool(xi)
        xi = torch.flatten(xi, 1)  # Flatten for fully connected layers
        xi = F.relu(self.fc1(xi))
        xi = F.relu(self.fc2(xi))
        xi = self.fc3(xi)
        return xi
    
    def set_outer_loop(self,outer_loop):
        self.outer_loop = outer_loop
    
    def set_lr1(self, learning_rate):
        self.lr1 = learning_rate
    
    
    
    #def initilization(self,x,eta):
        #self.x = x
        #self.eta = eta
    
    # for c(\zeta,\xi).
    
    # for different loss function ell.
    def mse_metric(self, predictions, targets):
        #x = x.requires_grad_(True)
        #device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        #n = x.size(1)
        #model = LinearModel(n,1,x)
        #model = model.to(device)
        #with torch.no_grad():
        
        #model.linear.weight =  nn.Parameter(x)
        # now assume bias = 0.
        #model.linear.bias = nn.Parameter(torch.tensor([0.0]))
        #predictions = model(inputs)
        return F.mse_loss(predictions, targets, reduction='mean')#.item()

    # Define Cross-Entropy metric
    def cross_entropy_metric(self,predictions, targets):
        return F.cross_entropy(predictions, targets, reduction='mean')#.item()
    
    def baseline_fDRO(self,predictions, eta,targets):
        """
        

        Parameters
        ----------
        predictions: should be a group of predictions, the number should be same as
        xi.
        x : model parameters
        eta : dual variable for $f$-divergence
        per_zeta : each zeta_sample.
        per_target : output y
        generated_xi: the xi samples generated by user(distribution shift).
        generated_target: the corresponding generated target by user.

        Raises
        ------
        ValueError
            DESCRIPTION.

        Returns
        -------
        inner_expectation : objective value of inner problem.
        grad: gradient w.r.t dual variable eta
        
        """
        
        
        m0 = predictions.shape[0]
        #x = nn.Parameter(x)
        #eta = eta.detach()
        #per_target = per_target.reshape((1,1))
        #n = per_zeta.size(0)
        #per_zeta = per_zeta.reshape((1,n))
        #xi_samples = xi_samples.to(self.device)
        #per_target = per_target.to(self.device)
        inner_values = torch.empty((m0,))
        for i in range(m0):
            #zeta = zeta_samples[i,:].unsqueeze(0)
            target = targets[i]
            prediction = predictions[i]
            # Select loss function
            #loss = nn.MSELoss()
            prediction = prediction.to(self.device)
            loss_value = self.cross_entropy_metric(prediction,target)
            
            #print(loss_value)
            # Compute the inner term
            quad_term = torch.div((loss_value - eta) 
                                 , (self.regularization))
            quad_value = 0.25*(torch.clamp(quad_term+2,min=0.0).pow(2))
            #exp_term_4_grad = exp_term.clone().detach()
            quad_term_obj = self.regularization*quad_value
            #print(exp_term_obj.size())
            inner_values[i]= quad_term_obj
            
            #inner_grad.append(exp_term_4_grad)

        # Mean of inner values (sample mean approximation of inner expectation)
        # apply full gradient descent in inner optimization.
        inner_expectation = inner_values.mean() + eta
        return inner_expectation
